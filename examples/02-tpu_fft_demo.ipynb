{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DifferentiableUniverseInitiative/jaxDecomp/blob/main/examples/tpu_fft_demo.ipynb)\n",
    "\n",
    "# jaxDecomp: Distributed 3D FFT on TPUs\n",
    "\n",
    "This notebook demonstrates `pfft3d` and `pifft3d` (distributed 3D FFTs) running on a **TPU runtime** in Google Colab.\n",
    "\n",
    "Unlike the CPU-based `demo_features.ipynb` which simulates multiple devices, this notebook uses **real TPU cores** for truly distributed computation.\n",
    "\n",
    "> **Setup:** You must select a TPU runtime before running this notebook.  \n",
    "> Go to **Runtime > Change runtime type > TPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jaxdecomp\n",
    "\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.sharding import AxisType, NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "import jaxdecomp as jdp\n",
    "\n",
    "devices = jax.devices()\n",
    "print(f'Backend:      {jax.default_backend()}')\n",
    "print(f'Device count: {jax.device_count()}')\n",
    "print(f'Devices:      {devices}')\n",
    "\n",
    "assert jax.device_count() > 1, 'Only 1 device found. Make sure you selected a TPU runtime: ' 'Runtime > Change runtime type > TPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pdims from device count\n",
    "# For 8 TPU cores: (4, 2) pencil decomposition\n",
    "n_devices = jax.device_count()\n",
    "\n",
    "# Find a balanced 2D factorization\n",
    "import math\n",
    "\n",
    "py = int(math.isqrt(n_devices))\n",
    "while n_devices % py != 0:\n",
    "    py -= 1\n",
    "pz = n_devices // py\n",
    "pdims = (pz, py)\n",
    "\n",
    "mesh = jax.make_mesh(\n",
    "    pdims,\n",
    "    ('z', 'y'),\n",
    "    axis_types=(AxisType.Auto, AxisType.Auto),\n",
    ")\n",
    "\n",
    "print(f'Device count: {n_devices}')\n",
    "print(f'Mesh pdims:   {pdims}')\n",
    "print(f'Mesh shape:   {mesh.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed 3D FFT\n",
    "\n",
    "`pfft3d` computes a distributed 3D FFT. It performs local FFTs on each TPU core and uses `all_to_all` transposes to redistribute data between axes.\n",
    "\n",
    "**Important:** The output shape is **transposed** relative to the input: `(X, Y, Z) -> (Y, Z, X)`. This is because the data must be rearranged across devices during the FFT.\n",
    "\n",
    "`pifft3d` computes the inverse, restoring the original shape and layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_shape = (512, 512, 512)\n",
    "\n",
    "sharding = NamedSharding(mesh, P('z', 'y'))\n",
    "key = jax.random.PRNGKey(42)\n",
    "x = jax.device_put(jax.random.normal(key, global_shape), sharding)\n",
    "\n",
    "# Forward FFT\n",
    "k = jdp.pfft3d(x)\n",
    "\n",
    "print(f'Input shape:  {x.shape}')\n",
    "print(f'Output shape: {k.shape}  (transposed: X,Y,Z -> Y,Z,X)')\n",
    "print(f'Output dtype: {k.dtype}')\n",
    "\n",
    "print('\\nInput sharding:')\n",
    "jax.debug.visualize_array_sharding(x[..., 0])\n",
    "\n",
    "print('\\nFFT output sharding:')\n",
    "jax.debug.visualize_array_sharding(k[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse FFT: recover the original array\n",
    "x_rec = jdp.pifft3d(k)\n",
    "\n",
    "print(f'Reconstructed shape: {x_rec.shape}  (matches original: {x_rec.shape == x.shape})')\n",
    "\n",
    "# Verify round-trip correctness\n",
    "is_close = jnp.allclose(x, x_rec.real, atol=1e-5)\n",
    "print(f'Round-trip FFT -> IFFT allclose: {is_close}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing\n",
    "\n",
    "JAX uses JIT compilation, so the **first call** includes compilation overhead. Subsequent calls execute the cached compiled program and are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pfft3d_jit = jax.jit(jdp.pfft3d)\n",
    "pifft3d_jit = jax.jit(jdp.pifft3d)\n",
    "\n",
    "# Warm up (includes compilation)\n",
    "k_warmup = pfft3d_jit(x).block_until_ready()\n",
    "_ = pifft3d_jit(k_warmup).block_until_ready()\n",
    "\n",
    "# Benchmark forward FFT\n",
    "n_iters = 10\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iters):\n",
    "    k_out = pfft3d_jit(x).block_until_ready()\n",
    "elapsed_fwd = (time.perf_counter() - start) / n_iters\n",
    "\n",
    "# Benchmark inverse FFT\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iters):\n",
    "    x_out = pifft3d_jit(k_out).block_until_ready()\n",
    "elapsed_inv = (time.perf_counter() - start) / n_iters\n",
    "\n",
    "print(f'Array shape: {global_shape}')\n",
    "print(f'Mesh pdims:  {pdims}')\n",
    "print(f'pfft3d:  {elapsed_fwd*1000:.1f} ms / call  ({n_iters} iterations)')\n",
    "print(f'pifft3d: {elapsed_inv*1000:.1f} ms / call  ({n_iters} iterations)')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
