{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Parallelism with jaxDecomp\n",
    "\n",
    "This notebook demonstrates how to perform batched (or nested parallelism) FFTs using `jaxDecomp`.\n",
    "We will utilize `jax.vmap` to vectorize over a chain axis, allowing JAX to handle batch parallelism while `jaxDecomp` handles the distributed FFTs within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:19.943370Z",
     "iopub.status.busy": "2026-02-07T14:32:19.942962Z",
     "iopub.status.idle": "2026-02-07T14:32:20.224896Z",
     "shell.execute_reply": "2026-02-07T14:32:20.224436Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=16\"  # Force 16 virtual devices for simulation\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import numpy as np\n",
    "import jaxdecomp.fft as jdfft\n",
    "from functools import partial\n",
    "from jax.experimental.multihost_utils import process_allgather # Helper to gather data from all devices to host\n",
    "gather = partial(process_allgather, tiled=True) # Helper to gather data from all devices to host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Full Mesh\n",
    "\n",
    "We treat the 16 devices as 2 Groups (c) of 8 Devices (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:20.227045Z",
     "iopub.status.busy": "2026-02-07T14:32:20.226900Z",
     "iopub.status.idle": "2026-02-07T14:32:20.302293Z",
     "shell.execute_reply": "2026-02-07T14:32:20.301858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:2026-02-07 15:32:20,296:jax._src.xla_bridge:491: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wassim/micromamba/envs/ffi11/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 489, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "  File \"/home/wassim/micromamba/envs/ffi11/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 328, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "  File \"/home/wassim/micromamba/envs/ffi11/lib/python3.11/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 285, in _check_cuda_versions\n",
      "    local_device_count = cuda_versions.cuda_device_count()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "mesh = Mesh(np.array(devices).reshape(2, 4, 2), (\"c\", \"x\", \"y\")) # 2 groups * 8 devices/group = 16 total\n",
    "mesh_inner = Mesh(np.array(devices[:8]).reshape(4, 2), (\"x\", \"y\"))  # For per-group logic # 2 groups * 8 devices/group = 16 total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Per-Group Logic\n",
    "\n",
    "This function expects a SINGLE group data (Chain dimension is stripped).\n",
    "It uses standard Auto-Sharding tools (`jdfft`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:20.322875Z",
     "iopub.status.busy": "2026-02-07T14:32:20.322706Z",
     "iopub.status.idle": "2026-02-07T14:32:20.325159Z",
     "shell.execute_reply": "2026-02-07T14:32:20.324739Z"
    }
   },
   "outputs": [],
   "source": [
    "def per_group_logic(arr):\n",
    "    # arr shape: (Z, Y, X)\n",
    "    # jdfft expects a global-looking array to apply its constraints.\n",
    "    # Debug print to verify sharding inside the kernel\n",
    "    jax.debug.inspect_array_sharding(arr , callback=lambda sharding: print(f\"Sharding in per_group_logic: {sharding}\"))\n",
    "    print(f\"shape here is {arr.shape}\")\n",
    "    # Perform 3D FFT on the sharded array (distributed across x and y)\n",
    "    return jdfft.pfft3d(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Batched Operation\n",
    "\n",
    "We use `vmap` to vectorize over the chain axis.\n",
    "JAX compiler automatically handles the Batch Parallelism for c and the Tensor Parallelism for x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:20.326319Z",
     "iopub.status.busy": "2026-02-07T14:32:20.326208Z",
     "iopub.status.idle": "2026-02-07T14:32:20.328293Z",
     "shell.execute_reply": "2026-02-07T14:32:20.327925Z"
    }
   },
   "outputs": [],
   "source": [
    "batched_op = jax.jit(jax.vmap(per_group_logic)) # JIT compile the vmapped function for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run\n",
    "\n",
    "We initialize global data with shape (Chain=2, Z=32, Y=32, X=32) and apply sharding constraints.\n",
    "JAX will run 2 distributed FFTs in parallel:\n",
    "- Group 0 handles Batch 0 (Distributed over its 8 GPUs)\n",
    "- Group 1 handles Batch 1 (Distributed over its 8 GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:20.329832Z",
     "iopub.status.busy": "2026-02-07T14:32:20.329720Z",
     "iopub.status.idle": "2026-02-07T14:32:21.032458Z",
     "shell.execute_reply": "2026-02-07T14:32:21.031905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sharding: NamedSharding(mesh=Mesh('c': 2, 'x': 4, 'y': 2, axis_types=(Auto, Auto, Auto)), spec=PartitionSpec('c', 'x', 'y'), memory_kind=device)\n",
      "shape here is (32, 32, 32)\n",
      "Sharding in per_group_logic: NamedSharding(mesh=Mesh('c': 2, 'x': 4, 'y': 2, axis_types=(Auto, Auto, Auto)), spec=PartitionSpec('c', 'x', 'y'), memory_kind=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding in per_group_logic: NamedSharding(mesh=Mesh('x': 4, 'y': 2, axis_types=(Auto, Auto)), spec=PartitionSpec('x', 'y'), memory_kind=device)\n",
      "shape here is (32, 32, 32)\n",
      "Output Shape: (2, 32, 32, 32)\n",
      "Output Sharding: NamedSharding(mesh=Mesh('c': 2, 'x': 4, 'y': 2, axis_types=(Auto, Auto, Auto)), spec=PartitionSpec('c', 'x', 'y'), memory_kind=device)\n"
     ]
    }
   ],
   "source": [
    "# Global Data: (Chain=2, Z=32, Y=32, X=32)\n",
    "global_data = jax.random.normal(jax.random.key(0), (2, 32, 32, 32)) # Create random data on host\n",
    "\n",
    "# Constraint: Shard C over c, Z over x, Y over y\n",
    "input_sharding = NamedSharding(mesh, P(\"c\", \"x\", \"y\")) # Define global sharding: partitioned along c, x, y\n",
    "sharded_global_data = jax.lax.with_sharding_constraint(global_data, input_sharding) # Create random data on host\n",
    "\n",
    "print(f\"Input Sharding: {sharded_global_data.sharding}\")\n",
    "\n",
    "# Execute\n",
    "# Run the batched FFT\n",
    "output = batched_op(sharded_global_data)\n",
    "\n",
    "# Extract first batch element for verification\n",
    "inner = global_data[0]  # Shape: (Z=32, Y=32, X=32) for the first group\n",
    "inner = jax.lax.with_sharding_constraint(inner, NamedSharding(mesh_inner, P(\"x\", \"y\"))) # Define global sharding: partitioned along c, x, y\n",
    "# Run the batched FFT\n",
    "single_group_output = per_group_logic(inner)\n",
    "\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Output Sharding: {output.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification\n",
    "\n",
    "We verify if the batched output is consistent with the result from a single group logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:21.034625Z",
     "iopub.status.busy": "2026-02-07T14:32:21.034429Z",
     "iopub.status.idle": "2026-02-07T14:32:21.082765Z",
     "shell.execute_reply": "2026-02-07T14:32:21.082186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is output close to single group result? True\n"
     ]
    }
   ],
   "source": [
    "batched_out = gather(output) # Collect distributed results to host for comparison\n",
    "single_out = gather(single_group_output) # Collect distributed results to host for comparison\n",
    "\n",
    "print(f\"is output close to single group result? {jnp.allclose(batched_out[0], single_out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also checkout the sharding of the output to undestand more\n",
    "in the next cell this is a 2D slice of the batch axis and the first of the sharding axes (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:21.084659Z",
     "iopub.status.busy": "2026-02-07T14:32:21.084483Z",
     "iopub.status.idle": "2026-02-07T14:32:21.233476Z",
     "shell.execute_reply": "2026-02-07T14:32:21.232891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m                    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m                    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m                    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m                    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m                    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m                    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m                    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(output[..., 0 , 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the sharding of the output and split it on the batch axis we can see that there are pairs of groups : \n",
    "\n",
    "```\n",
    "0 - 8\n",
    "1 - 9\n",
    "2 - 10\n",
    "etc ...\n",
    "```\n",
    "\n",
    "That run independently the same logic on different batches. Each group is sharding the data across the z and y axis as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:21.234801Z",
     "iopub.status.busy": "2026-02-07T14:32:21.234624Z",
     "iopub.status.idle": "2026-02-07T14:32:21.296419Z",
     "shell.execute_reply": "2026-02-07T14:32:21.295890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,8   </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1,9   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2,10  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3,11  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4,12  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5,13  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6,14  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7,15  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,8\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1,9\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2,10\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3,11\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4,12\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5,13\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6,14\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7,15\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(output[0, ... , 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HLO Inspection\n",
    "\n",
    "We inspect the HLO to verify the independence of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:21.298273Z",
     "iopub.status.busy": "2026-02-07T14:32:21.298166Z",
     "iopub.status.idle": "2026-02-07T14:32:21.301669Z",
     "shell.execute_reply": "2026-02-07T14:32:21.301182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant HLO instruction showing independent groups:\n",
      "%all-to-all = (c64[1,8,16,16]{3,2,1,0}, c64[1,8,16,16]{3,2,1,0}) all-to-all(%wrapped_slice, %wrapped_slice.1), channel_id=1, replica_groups={{0,1},{2,3},{4,5},{6,7},{8,9},{10,11},{12,13},{14,15}}, metadata={op_name=\"jit(per_group_logic)/vmap(jit(_do_pfft))/jit(pfft_impl)/custom_partitioning\" stack_frame_id=12}\n",
      "%all-to-all.1 = (c64[1,16,8,8]{3,2,1,0}, c64[1,16,8,8]{3,2,1,0}, c64[1,16,8,8]{3,2,1,0}, c64[1,16,8,8]{3,2,1,0}) all-to-all(%wrapped_slice.2, %wrapped_slice.3, %wrapped_slice.4, %wrapped_slice.5), channel_id=1, replica_groups={{0,2,4,6},{1,3,5,7},{8,10,12,14},{9,11,13,15}}, metadata={op_name=\"jit(per_group_logic)/vmap(jit(_do_pfft))/jit(pfft_impl)/custom_partitioning\" stack_frame_id=12}\n"
     ]
    }
   ],
   "source": [
    "HLO = batched_op.lower(sharded_global_data).compile().as_text()\n",
    "# We look for the all-to-all instruction which handles the FFT data redistribution\n",
    "# The replica_groups should show that devices communicate only within their group (c dimension)\n",
    "print(\"Relevant HLO instruction showing independent groups:\")\n",
    "found = False\n",
    "for line in HLO.split(\"\\n\"):\n",
    "    if \"all-to-all\" in line and \"replica_groups\" in line:\n",
    "        print(line.strip())\n",
    "        found = True\n",
    "if not found:\n",
    "    print(\"No all-to-all with replica_groups found. Check HLO for other communication primitives.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
